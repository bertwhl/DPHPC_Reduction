{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href=\"https://fonts.googleapis.com/icon?family=Material+Icons\" rel=\"stylesheet\">\n",
       "        <script>\n",
       "        require.config({\n",
       "            paths: {\n",
       "                \"math\": \"https://spcl.github.io/dace/webclient/external_lib/math.min\"\n",
       "            },\n",
       "            waitSeconds: 40\n",
       "          });\n",
       "        require( [\"math\"], x => window.math = x);\n",
       "        </script><script src=\"https://spcl.github.io/dace/webclient/renderer_dir/dagre.js\"></script>\n",
       "<script src=\"https://spcl.github.io/dace/webclient/renderer_dir/global_vars.js\"></script>\n",
       "<script src=\"https://spcl.github.io/dace/webclient/context_menu.js\"></script>\n",
       "<script src=\"https://spcl.github.io/dace/webclient/renderer_elements.js\"></script>\n",
       "<script src=\"https://spcl.github.io/dace/webclient/sdfg_utils.js\"></script>\n",
       "<script src=\"https://spcl.github.io/dace/webclient/overlay_manager.js\"></script>\n",
       "<script src=\"https://spcl.github.io/dace/webclient/renderer.js\"></script>\n",
       "<link href=\"https://spcl.github.io/dace/webclient/sdfv.css\" rel=\"stylesheet\">\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dace\n",
    "import numpy as np\n",
    "from dace.transformation.interstate import GPUTransformSDFG\n",
    "from dace.frontend.common import op_repository as oprepo\n",
    "\n",
    "@oprepo.replaces('warpReduce_sum')\n",
    "def warpReduce_sum(pv, sdfg: dace.SDFG, state: dace.SDFGState, x: str) -> str:\n",
    "   desc = sdfg.arrays[x]\n",
    "   newname, _ = sdfg.add_temp_transient(desc.shape, desc.dtype, desc.storage)\n",
    "   ctype = desc.dtype.ctype\n",
    "\n",
    "   t = state.add_tasklet(\n",
    "       'warpReduce', {'__a'}, {'__out'}, f'''\n",
    "       __out = dace::warpReduce<dace::ReductionType::Sum, {ctype}>::reduce(__a);\n",
    "   ''', dace.Language.CPP)\n",
    "   r = state.add_read(x)\n",
    "   w = state.add_write(newname)\n",
    "   state.add_edge(r, None, t, '__a', dace.Memlet(data=x))\n",
    "   state.add_edge(t, '__out', w, None, dace.Memlet(data=newname))\n",
    "   return newname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 128x128 -> 1x128 using 8 blocks with method WARP READ WARP REDUCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dace.program\n",
    "def reduce_test1(inputs: dace.float64[128, 128]):\n",
    "    outputs = dace.ndarray([128], dtype=dace.float64)\n",
    "    outputs[:] = 0\n",
    "    for blockIdx_y, blockIdx_x in dace.map[0:2, 0:4]:\n",
    "        shared = dace.ndarray([32,32], dtype=dace.float64, storage=dace.StorageType.GPU_Shared)\n",
    "        for threadIdx_y, threadIdx_x in dace.map[0:32,0:32]:\n",
    "            value = dace.float64(0)\n",
    "            for i in dace.map[0:2]:\n",
    "                value += inputs[64*i+32*blockIdx_y+threadIdx_y, 32*blockIdx_x+threadIdx_x]\n",
    "            shared[threadIdx_x,threadIdx_y] = value\n",
    "        for threadIdx_y, threadIdx_x in dace.map[0:32,0:32]:\n",
    "            reduced = warpReduce_sum(shared[threadIdx_y,threadIdx_x])\n",
    "            if threadIdx_x==0:\n",
    "                outputs[32*blockIdx_x+threadIdx_y] += reduced\n",
    "    return outputs\n",
    "\n",
    "# Transform to GPU, keep thread-block map\n",
    "sdfg = reduce_test1.to_sdfg()\n",
    "sdfg.apply_transformations(GPUTransformSDFG, {'sequential_innermaps': False})\n",
    "\n",
    "# Test\n",
    "a = np.random.rand(128, 128)\n",
    "b = sdfg(a)\n",
    "c = np.sum(a, axis=0)\n",
    "assert np.allclose(b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 128x128 -> 1x128 using 8 blocks with method THREAD REDUCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dace.program\n",
    "def reduce_test2(inputs: dace.float64[128, 128]):\n",
    "    outputs = dace.ndarray([128], dtype=dace.float64)\n",
    "    outputs[:] = 0\n",
    "    for blockIdx_y, blockIdx_x in dace.map[0:2, 0:4]:\n",
    "        shared = dace.ndarray([32,32], dtype=dace.float64, storage=dace.StorageType.GPU_Shared)\n",
    "        for threadIdx_y, threadIdx_x in dace.map[0:32,0:32]:\n",
    "            value = dace.float64(0)\n",
    "            for i in dace.map[0:2]:\n",
    "                value += inputs[64*i+32*blockIdx_y+threadIdx_y, 32*blockIdx_x+threadIdx_x]\n",
    "            shared[threadIdx_x,threadIdx_y] = value\n",
    "        for threadIdx_y, threadIdx_x in dace.map[0:32,0:32]:\n",
    "            reduced = warpReduce_sum(shared[threadIdx_y,threadIdx_x])\n",
    "            if threadIdx_x==0:\n",
    "                outputs[32*blockIdx_x+threadIdx_y] += reduced\n",
    "    return outputs\n",
    "\n",
    "# Transform to GPU, keep thread-block map\n",
    "sdfg = reduce_test2.to_sdfg()\n",
    "sdfg.apply_transformations(GPUTransformSDFG, {'sequential_innermaps': False})\n",
    "\n",
    "# Test\n",
    "a = np.random.rand(128, 128)\n",
    "b = sdfg(a)\n",
    "c = np.sum(a, axis=0)\n",
    "assert np.allclose(b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8x4096 -> 8x1 using 8 blocks with Multiple Blocks per Row (use atomic add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dace.program\n",
    "def reduce_test3(inputs: dace.float64[8, 4096]):\n",
    "    outputs = dace.ndarray([4096], dtype=dace.float64)\n",
    "    outputs[:] = 0\n",
    "    for blockIdx_y, blockIdx_x in dace.map[0:2, 0:4]:\n",
    "        for threadIdx_y, threadIdx_x in dace.map[0:32,0:32]:\n",
    "            value = dace.float64(0)\n",
    "            for i in dace.map[0:4]:\n",
    "                value += inputs[2*i+blockIdx_y, 1024*blockIdx_x+32*threadIdx_y+threadIdx_x]\n",
    "            outputs[1024*blockIdx_x+32*threadIdx_y+threadIdx_x] += value\n",
    "    return outputs\n",
    "\n",
    "# Transform to GPU, keep thread-block map\n",
    "sdfg = reduce_test3.to_sdfg()\n",
    "sdfg.apply_transformations(GPUTransformSDFG, {'sequential_innermaps': False})\n",
    "\n",
    "# Test\n",
    "a = np.random.rand(8, 4096)\n",
    "b = sdfg(a)\n",
    "c = np.sum(a, axis=0)\n",
    "assert np.allclose(b, c)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a454077445cb40177eb70c7757e8ad5629f6f8df50cd7e5eb3efe32691a1c3a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
