{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Wraps per Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dace\n",
    "import numpy as np\n",
    "from dace.transformation.interstate import GPUTransformSDFG, StateFusion\n",
    "from dace.frontend.common import op_repository as oprepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "@oprepo.replaces('warpReduce_sum')\n",
    "def warpReduce_sum(pv, sdfg: dace.SDFG, state: dace.SDFGState, x: str) -> str:\n",
    "   desc = sdfg.arrays[x]\n",
    "   newname, _ = sdfg.add_temp_transient(desc.shape, desc.dtype, desc.storage)\n",
    "   ctype = desc.dtype.ctype\n",
    "\n",
    "   t = state.add_tasklet(\n",
    "       'warpReduce', {'__a'}, {'__out'}, f'''\n",
    "       __out = dace::warpReduce<dace::ReductionType::Sum, {ctype}>::reduce(__a);\n",
    "   ''', dace.Language.CPP)\n",
    "   r = state.add_read(x)\n",
    "   w = state.add_write(newname)\n",
    "   state.add_edge(r, None, t, '__a', dace.Memlet(data=x))\n",
    "   state.add_edge(t, '__out', w, None, dace.Memlet(data=newname))\n",
    "   return newname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = dace.symbol('H')\n",
    "W = dace.symbol('W')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 120x640 -> 120x1 using 120 blocks, 20 warps each block, one block for one row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dace.program\n",
    "def AB_MWPR_1(inputs: dace.float64[H, W], wn: dace.int64):\n",
    "    outputs = dace.ndarray([H], dtype=dace.float64)\n",
    "    outputs[:] = 0\n",
    "    for block_id in dace.map[0:H]:\n",
    "        for warp_id, thread_id in dace.map[0:wn, 0:32]:\n",
    "            col = warp_id*32+thread_id\n",
    "            reduced = warpReduce_sum(inputs[block_id, col])\n",
    "            if thread_id == 0:\n",
    "                outputs[block_id] += reduced\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdfg1 = AB_MWPR_1.to_sdfg()\n",
    "sdfg1.apply_transformations(GPUTransformSDFG, {'sequential_innermaps': False})\n",
    "\n",
    "h, w = 120, 640\n",
    "a1 = np.random.rand(h, w)\n",
    "warp_num = w // 32\n",
    "b1 = sdfg1(H = h, W = w, inputs = a1, wn = warp_num)\n",
    "c1 = np.sum(a1, axis=1)\n",
    "assert np.allclose(b1, c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dace.program\n",
    "def AtomicReduceToGlobalMem(inputs: dace.float64[H, W], \n",
    "#                             num_blocks_per_row: dace.int64, \n",
    "#                             loopNum: dace.int64, \n",
    "                            blockDim_y: dace.int64):\n",
    "    outputs = dace.ndarray([H], dtype=dace.float64)\n",
    "    outputs[:] = 0\n",
    "#     for blockIdx_y, blockIdx_x in dace.map[0:num_blocks_per_row, 0:H]:\n",
    "    for blockIdx_x in dace.map[0:H]:\n",
    "        for warp_id, thread_id in dace.map[0:blockDim_y,0:32]:\n",
    "#             row_id = blockIdx_x\n",
    "#             col_id = 32*blockDim_y*blockIdx_y + 32*warp_id +thread_id\n",
    "#             value = dace.float64(0)\n",
    "#             delta = 32*blockDim_y*num_blocks_per_row\n",
    "#             for loopIdx in dace.map[0:loopNum]:\n",
    "#                 if col_id<W:\n",
    "#                     value += inputs[row_id, col_id]\n",
    "#                 col_id += delta\n",
    "            col_id = warp_id*32+thread_id\n",
    "            reduced = warpReduce_sum(inputs[blockIdx_x, col_id])\n",
    "            if thread_id == 0:\n",
    "                outputs[blockIdx_x] += reduced\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdfg = AtomicReduceToGlobalMem.to_sdfg()\n",
    "sdfg.apply_transformations(GPUTransformSDFG, {'sequential_innermaps': False})\n",
    "row = 12\n",
    "col = 640\n",
    "num_blocks_per_row = 1\n",
    "loopNum = 1\n",
    "WarpNum = 20\n",
    "test_input = np.random.rand(row, col)\n",
    "test_output = sdfg(H=row, W=col, inputs=test_input, \n",
    "#                    num_blocks_per_row=num_blocks_per_row, loopNum=loopNum, \n",
    "                   blockDim_y=WarpNum)\n",
    "expected_output = np.sum(test_input, axis=1)\n",
    "assert np.allclose(test_output, expected_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 40x640 -> 40x1 using 25 blocks, 32 warps each block, 20 wraps for one row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dace.program\n",
    "def AB_MWPR_2(inputs: dace.float64[H, W], bn: dace.int64):\n",
    "    outputs = dace.ndarray([H], dtype=dace.float64)\n",
    "    outputs[:] = 0\n",
    "    for block_id in dace.map[0:bn]:\n",
    "        for warp_id, thread_id in dace.map[0:32, 0:32]:\n",
    "            index = block_id * 1024 + warp_id * 32 + thread_id\n",
    "            index_x = dace.int64(index/640)\n",
    "            index_y = index % 640\n",
    "            value = warpReduce_sum(inputs[index_x, index_y])\n",
    "            if thread_id == 0:\n",
    "                outputs[index_x] += value\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdfg2 = AB_MWPR_2.to_sdfg()\n",
    "sdfg2.apply_transformations(GPUTransformSDFG, {'sequential_innermaps': False})\n",
    "\n",
    "h, w = 120, 640\n",
    "block_num = h * w // 512\n",
    "\n",
    "a2 = np.random.rand(h, w)\n",
    "b2 = sdfg2(H = h, W = w, inputs = a2, bn = block_num)\n",
    "c2 = np.sum(a2, axis=1)\n",
    "assert np.allclose(b2, c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 120x640 -> 120x1 using 15 blocks, 32 warps each block, one block for 8 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dace.program\n",
    "def AB_MWPR_3(inputs: dace.float64[H, W], bn: dace.int64, ln: dace.int64, rpb: dace.int64, wpr: dace.int64):\n",
    "    outputs = dace.ndarray([H], dtype=dace.float64)\n",
    "    outputs[:] = 0\n",
    "    for block_id in dace.map[0:bn]:\n",
    "        _rpb = dace.int32(rpb)\n",
    "        _wpr = dace.int32(wpr)\n",
    "        shared = dace.ndarray([32,32], dtype=dace.float64, storage=dace.StorageType.GPU_Shared)\n",
    "        for warp_id_x, warp_id_y, thread_id in dace.map[0:rpb, 0:wpr, 0:32]:\n",
    "            value = dace.float64(0)\n",
    "            for i in dace.map[0:ln]:\n",
    "                value += inputs[block_id * _rpb + warp_id_x \n",
    "                    ,i * _wpr * 32 + warp_id_y * 32 + thread_id]\n",
    "            shared[warp_id_x*wpr+warp_id_y, thread_id] = value\n",
    "        reduced = dace.ndarray([32], dtype=dace.float64, storage=dace.StorageType.GPU_Shared) # should be rpb instead of 32, but stupid dace can't work with variables here...\n",
    "        for warp_id_x, warp_id_y, thread_id in dace.map[0:rpb, 0:wpr, 0:32]:\n",
    "            reduced[warp_id_x] = warpReduce_sum(shared[warp_id_x * wpr + warp_id_y, thread_id])\n",
    "            if thread_id == 0:\n",
    "                outputs[block_id * rpb + warp_id_x] += reduced[warp_id_x]\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdfg3 = AB_MWPR_3.to_sdfg()\n",
    "sdfg3.apply_transformations(GPUTransformSDFG, {'sequential_innermaps': False})\n",
    "\n",
    "h, w = 120, 640\n",
    "row_per_block = 8\n",
    "warp_per_row = 32 // row_per_block\n",
    "block_num = h // row_per_block\n",
    "loop_num = w * row_per_block // 1024\n",
    "\n",
    "a3 = np.random.rand(h, w)\n",
    "b3 = sdfg3(H = h, W = w, inputs = a3, bn = block_num, ln = loop_num, rpb = row_per_block, wpr = warp_per_row)\n",
    "c3 = np.sum(a3, axis=1)\n",
    "assert np.allclose(b3, c3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b392c89b2e748132e166ebaad61f7326e6f59e1c0abeae75f1ca197830f38773"
  },
  "kernelspec": {
   "display_name": "Python [conda env:dace]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
